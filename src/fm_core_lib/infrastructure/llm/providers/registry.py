"""
Centralized Provider Registry for LLM providers.

This module provides a central registry for managing LLM providers, their configurations,
and fallback strategies. It resolves the scattered configuration problem by providing
a single source of truth for provider management.
"""

import logging
import os
from typing import Dict, List, Optional, Type, Union

try:
    from dotenv import load_dotenv
    load_dotenv()  # Load environment variables when module is imported
except ImportError:
    pass  # dotenv not available, continue without it

from .base import BaseLLMProvider, ProviderConfig, LLMResponse
from .fireworks_provider import FireworksProvider
from .openai_provider import OpenAIProvider
from .groq_provider import GroqProvider
from .local_provider import LocalProvider
from .anthropic import AnthropicProvider
from .gemini import GeminiProvider
from .huggingface import HuggingFaceProvider


# Data-driven provider schema - single source of truth
PROVIDER_SCHEMA = {
    "fireworks": {
        "api_key_var": "FIREWORKS_API_KEY",
        "model_var": "FIREWORKS_MODEL", 
        "base_url_var": "FIREWORKS_API_BASE",
        "default_base_url": "https://api.fireworks.ai/inference/v1",
        "default_model": "accounts/fireworks/models/llama-v3p1-8b-instruct",
        "provider_class": FireworksProvider,
        "confidence_score": 0.9
        # max_retries and timeout loaded from settings.llm.max_retries and settings.llm.request_timeout
    },
    "openai": {
        "api_key_var": "OPENAI_API_KEY",
        "model_var": "OPENAI_MODEL",
        "base_url_var": "OPENAI_API_BASE", 
        "default_base_url": "https://api.openai.com/v1",
        "default_model": "gpt-4o",
        "provider_class": OpenAIProvider,
        "confidence_score": 0.85
        # max_retries and timeout loaded from settings
    },
    "local": {
        "api_key_var": None,  # No API key needed
        "model_var": "LOCAL_LLM_MODEL",
        "base_url_var": "LOCAL_LLM_BASE_URL",
        "default_base_url": "http://localhost:5000",  # Default llama.cpp server endpoint
        "default_model": "llama2-7b",     # Default model name
        "provider_class": LocalProvider,
        "max_retries": 1,  # Local typically needs fewer retries
        "timeout": 60,  # Local may need more time
        "confidence_score": 0.6
        # Note: Local provider has different defaults for retries/timeout than settings
    },
    "gemini": {
        "api_key_var": "GEMINI_API_KEY",
        "model_var": "GEMINI_MODEL",
        "base_url_var": "GEMINI_API_BASE",
        "default_base_url": "https://generativelanguage.googleapis.com/v1beta",
        "default_model": "gemini-1.5-pro",
        "provider_class": GeminiProvider,
        "confidence_score": 0.8
        # max_retries and timeout loaded from settings
    },
    "huggingface": {
        "api_key_var": "HUGGINGFACE_API_KEY",
        "model_var": "HUGGINGFACE_MODEL",
        "base_url_var": "HUGGINGFACE_API_URL",
        "default_base_url": "https://api-inference.huggingface.co/models",
        "default_model": "tiiuae/falcon-7b-instruct",
        "provider_class": HuggingFaceProvider,
        "confidence_score": 0.75
        # max_retries and timeout loaded from settings
    },
    "openrouter": {
        "api_key_var": "OPENROUTER_API_KEY",
        "model_var": "OPENROUTER_MODEL",
        "base_url_var": "OPENROUTER_API_BASE",
        "default_base_url": "https://openrouter.ai/api/v1",
        "default_model": "openrouter-default",
        "provider_class": OpenAIProvider,  # Compatible API
        "confidence_score": 0.8
        # max_retries and timeout loaded from settings
    },
    "anthropic": {
        "api_key_var": "ANTHROPIC_API_KEY",
        "model_var": "ANTHROPIC_MODEL",
        "base_url_var": "ANTHROPIC_API_BASE",
        "default_base_url": "https://api.anthropic.com/v1",
        "default_model": "claude-3-sonnet-20240229",
        "provider_class": AnthropicProvider,
        "confidence_score": 0.85
        # max_retries and timeout loaded from settings
    },
    "groq": {
        "api_key_var": "GROQ_API_KEY",
        "model_var": "GROQ_MODEL",
        "base_url_var": "GROQ_API_BASE",
        "default_base_url": "https://api.groq.com/openai/v1",
        "default_model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "provider_class": GroqProvider,
        "confidence_score": 0.88  # Groq is fast and reliable
        # max_retries and timeout loaded from settings
    }
}


class ProviderRegistry:
    """Central registry for managing LLM providers"""
    
    def __init__(self, settings=None):
        self.logger = logging.getLogger(__name__)
        
        # Get settings if not provided
        if settings is None:
            try:
                from faultmaven.config.settings import get_settings
                settings = get_settings()
            except:
                settings = None
        
        self.settings = settings
        self._providers: Dict[str, BaseLLMProvider] = {}
        self._fallback_chain: List[str] = []
        self._initialized = False
        
        # Don't initialize immediately - wait for first use
        # self._initialize_from_environment()
    
    def _ensure_initialized(self):
        """Ensure providers are initialized before use"""
        if not self._initialized:
            self.logger.info("üîç Lazy-initializing provider registry...")
            
            # Force reload environment variables
            try:
                from dotenv import load_dotenv
                import os
                
                # Get the current working directory and look for .env file
                cwd = os.getcwd()
                env_file = os.path.join(cwd, '.env')
                
                if os.path.exists(env_file):
                    self.logger.info(f"üîç Loading .env file from: {env_file}")
                    load_dotenv(env_file, override=True)
                    self.logger.info("üîç Environment variables reloaded from .env file")
                else:
                    self.logger.warning(f"üîç .env file not found at: {env_file}")
                
                # Also try to load from parent directory
                parent_env = os.path.join(os.path.dirname(cwd), '.env')
                if os.path.exists(parent_env):
                    self.logger.info(f"üîç Loading .env file from parent: {parent_env}")
                    load_dotenv(parent_env, override=True)
                    self.logger.info("üîç Environment variables reloaded from parent .env file")
                    
            except ImportError:
                self.logger.info("üîç dotenv not available, using system environment")
            
            self._initialize_from_settings()
            self._initialized = True
    
    def _initialize_from_settings(self):
        """Initialize providers based on settings configuration using schema"""
        if self.settings:
            # Use settings-based configuration
            self.logger.info(f"üîç Settings-based provider configuration:")
            self.logger.info(f"üîç CHAT_PROVIDER: {self.settings.llm.provider}")
            self.logger.info(f"üîç LOCAL_LLM_URL: {self.settings.llm.local_url or 'NOT_SET'}")
            self.logger.info(f"üîç LOCAL_LLM_MODEL: {self.settings.llm.local_model or 'NOT_SET'}")
            fireworks_key_preview = "NOT_SET"
            if self.settings.llm.fireworks_api_key:
                fireworks_key_preview = self.settings.llm.fireworks_api_key.get_secret_value()[:10] + "..."
            self.logger.info(f"üîç FIREWORKS_API_KEY: {fireworks_key_preview}")
            
            # Get primary provider from settings
            primary_provider = self.settings.llm.provider
        else:
            # No fallback - unified settings system is mandatory
            from faultmaven.models.exceptions import LLMProviderError
            raise LLMProviderError(
                "LLM provider registry requires unified settings system to be available",
                error_code="LLM_CONFIG_ERROR",
                context={"settings_available": self.settings is not None}
            )
        
        # Validate that primary_provider is in schema
        if primary_provider not in PROVIDER_SCHEMA:
            valid_options = list(PROVIDER_SCHEMA.keys())
            self.logger.error(
                f"‚ùå Invalid CHAT_PROVIDER: '{primary_provider}'. "
                f"Valid options: {valid_options}. Defaulting to 'local'"
            )
            primary_provider = "local"
        
        # Initialize all providers defined in schema
        for provider_name, schema in PROVIDER_SCHEMA.items():
            try:
                self.logger.info(f"üîç Attempting to initialize provider: {provider_name}")
                config = self._create_provider_config(provider_name, schema)
                if config:
                    self._initialize_provider(provider_name, config)
                    self.logger.info(f"‚úÖ Provider '{provider_name}' initialized successfully")
                else:
                    self.logger.warning(f"‚ö†Ô∏è Provider '{provider_name}' config returned None (skipped)")
            except Exception as e:
                self.logger.warning(f"‚ùå Failed to initialize provider {provider_name}: {e}")
        
        # Set up fallback chain with primary first
        self._setup_fallback_chain(primary_provider)
    
    def _create_provider_config(self, provider_name: str, schema: Dict) -> Optional[ProviderConfig]:
        """Create provider configuration from schema and settings/environment variables"""
        
        api_key = None
        model = None
        base_url = None
        
        if self.settings:
            # Use settings-based configuration
            if provider_name == "fireworks":
                api_key = self.settings.llm.fireworks_api_key.get_secret_value() if self.settings.llm.fireworks_api_key else None
                model = self.settings.llm.fireworks_model or schema["default_model"]
                base_url = self.settings.llm.fireworks_base_url or schema["default_base_url"]
            elif provider_name == "openai":
                api_key = self.settings.llm.openai_api_key.get_secret_value() if self.settings.llm.openai_api_key else None
                model = self.settings.llm.openai_model or schema["default_model"]
                base_url = self.settings.llm.openai_base_url or schema["default_base_url"]
            elif provider_name == "local":
                api_key = None  # Local doesn't need API key
                model = self.settings.llm.local_model
                base_url = self.settings.llm.local_url
            elif provider_name == "anthropic":
                api_key = self.settings.llm.anthropic_api_key.get_secret_value() if self.settings.llm.anthropic_api_key else None
                model = self.settings.llm.anthropic_model or schema["default_model"]
                base_url = self.settings.llm.anthropic_base_url or schema["default_base_url"]
            elif provider_name == "gemini":
                api_key = self.settings.llm.gemini_api_key.get_secret_value() if self.settings.llm.gemini_api_key else None
                model = self.settings.llm.gemini_model or schema["default_model"]
                base_url = self.settings.llm.gemini_base_url or schema["default_base_url"]
            elif provider_name == "huggingface":
                api_key = self.settings.llm.huggingface_api_key.get_secret_value() if self.settings.llm.huggingface_api_key else None
                model = self.settings.llm.huggingface_model or schema["default_model"]
                base_url = self.settings.llm.huggingface_base_url or schema["default_base_url"]
            elif provider_name == "openrouter":
                api_key = self.settings.llm.openrouter_api_key.get_secret_value() if self.settings.llm.openrouter_api_key else None
                model = self.settings.llm.openrouter_model or schema["default_model"]
                base_url = self.settings.llm.openrouter_base_url or schema["default_base_url"]
            elif provider_name == "groq":
                api_key = self.settings.llm.groq_api_key.get_secret_value() if self.settings.llm.groq_api_key else None
                model = self.settings.llm.groq_chat_model or schema["default_model"]
                base_url = self.settings.llm.groq_base_url or schema["default_base_url"]
            
            # Skip providers without required API keys (except local)
            if schema.get("api_key_var") and not api_key and provider_name != "local":
                self.logger.warning(
                    f"‚ö†Ô∏è Skipping provider '{provider_name}': "
                    f"API key '{schema['api_key_var']}' not found in settings"
                )
                return None
                
        else:
            # Fallback to environment variables when settings unavailable
            # Check if API key is required and available
            api_key_var = schema.get("api_key_var")
            if api_key_var:
                api_key = os.getenv(api_key_var)
                if not api_key:
                    # Skip providers without required API keys
                    return None
            
            # Get configuration values from environment or defaults
            model = os.getenv(schema["model_var"], schema["default_model"])
            base_url = os.getenv(schema.get("base_url_var", ""), schema["default_base_url"])
        
        # For local provider, require environment variables
        if provider_name == "local":
            if not model:
                self.logger.warning(f"‚ùå Local provider requires LOCAL_LLM_MODEL environment variable")
                return None
            if not base_url:
                self.logger.warning(f"‚ùå Local provider requires LOCAL_LLM_URL environment variable")
                return None
        
        # Debug configuration values
        self.logger.info(f"üîç Provider '{provider_name}' config:")
        self.logger.info(f"üîç   Model: {model} (from {schema.get('model_var', 'N/A')})")
        self.logger.info(f"üîç   Base URL: {base_url} (from {schema.get('base_url_var', 'default')})")
        self.logger.info(f"üîç   API Key: {'SET' if api_key else 'NOT_SET'}")

        # Get timeout and max_retries from schema or settings
        if self.settings:
            timeout = schema.get("timeout", self.settings.llm.request_timeout)
            max_retries = schema.get("max_retries", self.settings.llm.max_retries)
        else:
            # Fallback to environment or defaults
            timeout = schema.get("timeout", int(os.getenv("LLM_REQUEST_TIMEOUT", "30")))
            max_retries = schema.get("max_retries", int(os.getenv("LLM_MAX_RETRIES", "3")))
        
        self.logger.info(f"üîç   Timeout: {timeout}s")
        self.logger.info(f"üîç   Max Retries: {max_retries}")

        return ProviderConfig(
            name=provider_name,
            api_key=api_key,
            base_url=base_url,
            models=[model],
            max_retries=max_retries,
            timeout=timeout,
            confidence_score=schema["confidence_score"]
        )
    
    def _initialize_provider(self, name: str, config: ProviderConfig):
        """Initialize a single provider using schema"""
        if name not in PROVIDER_SCHEMA:
            self.logger.warning(f"Unknown provider in schema: {name}")
            return
        
        schema = PROVIDER_SCHEMA[name]
        provider_class = schema["provider_class"]
        
        try:
            provider = provider_class(config)
            
            if provider.is_available():
                self._providers[name] = provider
                self.logger.info(f"‚úÖ Provider '{name}' initialized successfully")
            else:
                self.logger.warning(f"‚ùå Provider '{name}' not available (missing config)")
        except Exception as e:
            self.logger.error(f"‚ùå Error creating provider '{name}': {e}")
    
    def _setup_fallback_chain(self, primary_provider: str):
        """Set up the provider fallback chain"""
        # Start with primary provider
        chain = [primary_provider] if primary_provider in self._providers else []

        # Check if strict mode is enabled
        strict_mode = os.getenv("STRICT_PROVIDER_MODE", "false").lower() == "true"

        if strict_mode:
            # In strict mode, only use the primary provider
            self.logger.info(f"üîí Strict provider mode enabled - using only '{primary_provider}', no fallbacks")
        else:
            # Add other available providers as fallbacks
            fallback_order = ["fireworks", "openai", "local"]
            for provider in fallback_order:
                if provider != primary_provider and provider in self._providers:
                    chain.append(provider)

        self._fallback_chain = chain
        if strict_mode and len(chain) == 1:
            self.logger.info(f"Provider chain (strict mode): {chain[0]} ONLY")
        else:
            self.logger.info(f"Provider fallback chain: {' -> '.join(chain)}")
    
    def register_provider(self, name: str, provider_class: Type[BaseLLMProvider]):
        """Register a custom provider class"""
        self._provider_classes[name] = provider_class
        self.logger.info(f"Registered custom provider class: {name}")
    
    def get_provider(self, name: str) -> Optional[BaseLLMProvider]:
        """Get a specific provider by name"""
        self._ensure_initialized()
        return self._providers.get(name)
    
    def get_available_providers(self) -> List[str]:
        """Get list of available provider names"""
        self._ensure_initialized()
        return list(self._providers.keys())
    
    def get_all_provider_names(self) -> List[str]:
        """Get list of all provider names defined in schema"""
        self._ensure_initialized()
        return list(PROVIDER_SCHEMA.keys())
    
    def get_fallback_chain(self) -> List[str]:
        """Get the current fallback chain"""
        self._ensure_initialized()
        return self._fallback_chain.copy()
    
    async def route_request(
        self,
        prompt: str,
        model: Optional[str] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7,
        confidence_threshold: float = 0.8,
        **kwargs
    ) -> LLMResponse:
        """Route request through the fallback chain until success"""
        self._ensure_initialized()

        """
        Route request through the fallback chain until success

        Args:
            prompt: Input prompt
            model: Specific model to use (optional)
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            confidence_threshold: Minimum confidence threshold
            **kwargs: Additional parameters

        Returns:
            LLMResponse from successful provider

        Raises:
            Exception: If all providers fail
        """
        last_error = None
        best_low_confidence_response = None
        best_low_confidence_score = 0.0

        for provider_name in self._fallback_chain:
            provider = self._providers.get(provider_name)
            if not provider:
                continue

            try:
                self.logger.info(f"Trying provider: {provider_name}")

                response = await provider.generate(
                    prompt=prompt,
                    model=model,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    **kwargs
                )

                # Check confidence threshold
                if response.confidence >= confidence_threshold:
                    self.logger.info(
                        f"‚úÖ Success with {provider_name} "
                        f"(confidence: {response.confidence:.2f})"
                    )
                    return response
                else:
                    # Log the actual response content for debugging
                    self.logger.warning(
                        f"‚ö†Ô∏è Low confidence from {provider_name} "
                        f"({response.confidence:.2f} < {confidence_threshold})"
                    )
                    self.logger.info(
                        f"üîç Low confidence response content from {provider_name}: "
                        f"{response.content[:200]}{'...' if len(response.content) > 200 else ''}"
                    )

                    # Keep track of the best low-confidence response
                    if response.confidence > best_low_confidence_score:
                        best_low_confidence_response = response
                        best_low_confidence_score = response.confidence
                        self.logger.info(
                            f"üìù Keeping {provider_name} as best low-confidence option "
                            f"(confidence: {response.confidence:.2f})"
                        )

                    continue

            except Exception as e:
                self.logger.warning(f"‚ùå Provider {provider_name} failed: {e}")
                last_error = e
                continue

        # If we have a low-confidence response, return it with appropriate metadata
        if best_low_confidence_response:
            self.logger.info(
                f"üéØ Returning best low-confidence response "
                f"(confidence: {best_low_confidence_score:.2f}) instead of failing completely"
            )
            # Add metadata to indicate this is a low-confidence response
            best_low_confidence_response.provider = f"{best_low_confidence_response.provider} (low-confidence)"
            return best_low_confidence_response

        # All providers failed completely
        error_msg = f"All providers failed. Last error: {last_error}"
        self.logger.error(error_msg)
        raise Exception(error_msg)
    
    def get_provider_status(self) -> Dict[str, Dict[str, any]]:
        """Get status information for all providers"""
        self._ensure_initialized()
        status = {}
        
        for name, provider in self._providers.items():
            status[name] = {
                "available": provider.is_available(),
                "models": provider.get_supported_models(),
                "confidence_score": provider.config.confidence_score,
                "in_fallback_chain": name in self._fallback_chain
            }
        
        return status


# Global registry instance
_registry = None


def get_registry(settings=None) -> ProviderRegistry:
    """Get the global provider registry instance"""
    global _registry
    if _registry is None:
        print("üîç Creating new ProviderRegistry instance...")
        _registry = ProviderRegistry(settings=settings)
        print("üîç ProviderRegistry instance created")
    return _registry


def reset_registry():
    """Reset the global registry (mainly for testing)"""
    global _registry
    _registry = None


def get_valid_provider_names() -> List[str]:
    """Get list of valid provider names for CHAT_PROVIDER"""
    return list(PROVIDER_SCHEMA.keys())


def print_provider_options():
    """Print all valid provider options with descriptions"""
    print("Valid CHAT_PROVIDER options:")
    for name, schema in PROVIDER_SCHEMA.items():
        provider_class = schema["provider_class"].__name__
        default_model = schema["default_model"]
        print(f'  "{name}" - {provider_class} ({default_model})')
    print(f"\nExample: CHAT_PROVIDER=\"fireworks\"")